{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import myUnitsCopy1 as myU # customized library for units. All dimensional variables are in GeV and GeV=1\n",
    "import pandas as pd\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.coordinates import *\n",
    "import astropy.units as u\n",
    "\n",
    "from os import listdir\n",
    "import gzip\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Single EDR3 File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "edr3_data = './edr3_data'\n",
    "dr2_data = './dr2_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dr3_files = listdir(edr3_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "healpix_edr3_start = np.empty((len(list_dr3_files)),dtype= int)\n",
    "healpix_edr3_end = np.empty((len(list_dr3_files)), dtype = int)\n",
    "\n",
    "for i,file in enumerate(list_dr3_files):\n",
    "    int_1 = int(file[11:17])\n",
    "    int_2 = int(file[18:24])\n",
    "    healpix_edr3_start[i] = int_1\n",
    "    healpix_edr3_end[i] = int_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_ids(file_names):\n",
    "    #given a list of EDR3 filenames, return the start and end source IDs corresponding to healpix level 12\n",
    "    N_8 = 2**(59-16)\n",
    "    \n",
    "    start = np.array([x*N_8 for x in healpix_edr3_start], dtype = 'int')\n",
    "    end = np.array([x*N_8 for x in healpix_edr3_end], dtype = 'int')\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dr3_file(idx):\n",
    "    return pd.read_csv(edr3_data + '/' + list_dr3_files[idx], compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = get_source_ids(list_dr3_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Load Corresponding DR2 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dr2_files = np.array([file for file in listdir(dr2_data) if file[-7:]=='.csv.gz']) #select only files ending with 'csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dr2_files(idx):\n",
    "    #separate into two arrays of start/end source IDs\n",
    "    strings = np.array([file.split('_') for file in list_dr2_files])\n",
    "    sid_dr2_start = np.array([int(name) for name in strings[:,1]])\n",
    "    sid_dr2_end = np.array([int(name[:-7]) for name in strings[:,2]])\n",
    "\n",
    "    pass1 = np.where(start[idx] < sid_dr2_end)[0]\n",
    "    pass2 = np.where(end[idx] < sid_dr2_start)[0]\n",
    "\n",
    "    file_indices = np.setdiff1d(pass1, pass2)\n",
    "\n",
    "    files_to_open = list_dr2_files[file_indices]\n",
    "    print(str(len(files_to_open))+ ' corresponding files')\n",
    "    return pd.concat((pd.read_csv(dr2_data+ '/' + str(f), compression = 'gzip') for f in files_to_open))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in the eDR3 file index from the command line\n",
    "current_index = int(sys.argv[1]) # current index in list of edr3 files\n",
    "print('\\nReading in eDR3 file '+str(current_index)+'.'); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 corresponding files\n"
     ]
    }
   ],
   "source": [
    "dr3 = load_dr3_file(current_index)\n",
    "dr2 = load_dr2_files(current_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Pair Catalogue\n",
    "\n",
    "For each star, we first locate accidental pairs by on-sky proximity. This is the condition\n",
    "$$|\\theta_i - \\theta_j| < \\theta_\\text{min}$$\n",
    "where $i$ is the index of the foreground star, and $j$ is a background star for a given foreground $i$.\n",
    "After this first cut, we then impose that the background candidate be behind the foreground at $n_\\sigma$. \n",
    "\n",
    "$$\\varpi_i - \\varpi_j > n_\\sigma \\sqrt{\\sigma_{\\varpi_i}^2 + \\sigma_{\\varpi_j}^2}.$$\n",
    "\n",
    "When $n_\\sigma =2$, this corresponds to a 95% confidence level. We can tighten or relax these cuts in order to control the size/purity of the resulting pair catalogue.\n",
    "\n",
    "The above assumes that $\\sigma_{\\varpi_i}$ and $\\sigma_{\\varpi_j}$ have zero correlation. A stricter condition would be to assume that they had correlation = 1. If so, then the above formula becomes \n",
    "$$\\varpi_i - n_\\sigma \\sigma_{\\varpi_i} > \\varpi_i + n_\\sigma \\sigma_{\\varpi_j}.$$\n",
    "This results in fewer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pair_cat(df, angle_cutoff, n_sigma):\n",
    "    #Note that angle_cutoff is measured in arcseconds.\n",
    "    \n",
    "    ra_arr = np.asarray(df['ra'])\n",
    "    dec_arr = np.asarray(df['dec'])\n",
    "    coord1 = SkyCoord(ra_arr, dec_arr, unit = u.degree)\n",
    "    \n",
    "    #Search df for on-sky neighbors within angle_cutoff arsec\n",
    "    z = search_around_sky(coord1, coord1, angle_cutoff*u.arcsec, storekdtree = False)\n",
    "    \n",
    "    #The above snippet will count a foreground star as its own neighbor, so we must remove them:\n",
    "    idx = z[0][z[0] != z[1]]\n",
    "    dub = z[1][z[0] != z[1]]\n",
    "    \n",
    "    df_fore = df.iloc[idx]\n",
    "    df_back = df.iloc[dub]\n",
    "    \n",
    "    df_fore.reset_index(inplace = True, drop=True)\n",
    "    df_back.reset_index(inplace = True, drop=True)\n",
    "    \n",
    "    #Define a function to iterate over the foreground/background df's and check if they satisfy the parallax condition\n",
    "\n",
    "    is_behind = lambda par1, par2, err1, err2 : par1-par2 > n_sigma*np.sqrt(err1**2 + err2**2)\n",
    "    is_behind_list = is_behind(df_fore['parallax'], df_back['parallax'], df_fore['parallax_error'], df_back['parallax_error'])\n",
    "    \n",
    "    #Keep pairs that satisfy the parallax condition within n_sigma. \n",
    "    df_fore = df_fore[is_behind_list]\n",
    "    df_back = df_back[is_behind_list]\n",
    "    \n",
    "    #Concatenate the foreground and background list into one catalogue.\n",
    "    new_fg_cols = [x+\"_fg\" for x in df_fore.columns]\n",
    "    df_fore.columns= new_fg_cols\n",
    "    \n",
    "    new_bg_cols = [x+\"_bg\" for x in df_back.columns]\n",
    "    df_back.columns= new_bg_cols\n",
    "    \n",
    "    pair_cat = pd.concat([df_fore,df_back], axis = 1)\n",
    "    pair_cat.reset_index(inplace =True, drop = True)\n",
    "    return pair_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Acceleration Catalogue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs_list(dr3, dr2):\n",
    "    ra_arr1 = np.asarray(dr3['ra'])\n",
    "    dec_arr1 = np.asarray(dr3['dec'])\n",
    "\n",
    "    ra_arr2 = np.asarray(dr2['ra'])\n",
    "    dec_arr2 = np.asarray(dr2['dec'])\n",
    "    \n",
    "    coord1 = SkyCoord(ra_arr1, dec_arr1, unit = u.degree)\n",
    "    coord2 = SkyCoord(ra_arr2, dec_arr2, unit = u.degree)\n",
    "    \n",
    "    z = search_around_sky(coord1, coord2, 3*u.arcsec, storekdtree = False)\n",
    "    \n",
    "    df1 = dr3.iloc[z[0]]\n",
    "    df2 = dr2.iloc[z[1]]\n",
    "    \n",
    "    df1.reset_index(inplace = True, drop=True)\n",
    "    df2.reset_index(inplace = True, drop=True)\n",
    "    \n",
    "    new_cols = [x+\".1\" for x in df2.columns]\n",
    "    df2.columns= new_cols\n",
    "    result = pd.concat([df1,df2], axis = 1)\n",
    "    result = result[(result['astrometric_params_solved']>= 27) & (result['astrometric_params_solved.1']>= 27)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_back_linear(ra_g3, dec_g3, pmra_g3, pmdec_g3):\n",
    "    \"\"\"Takes EDR3 position and proper motion, and linearly propagates it by 0.5 year to the DR2 epoch. Output: SkyCoord object in DR2 epoch. Does not take into account parallax.\"\"\"\n",
    "    c = SkyCoord(ra = ra_g3 * u.deg, \n",
    "                 dec = dec_g3 * u.deg, \n",
    "                 distance = 1 * u.kpc, #setting distance to 1 kpc, otherwise it thinks stuff is at 10 Mpc and then returns an exception due to faster than light\n",
    "                 pm_ra_cosdec = pmra_g3 * u.mas/u.yr,\n",
    "                 pm_dec = pmdec_g3 * u.mas/u.yr,\n",
    "                 obstime = Time(2016.0, format='jyear'))\n",
    "    return c.apply_space_motion(Time(2015.5, format='jyear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm(pairs_list):\n",
    "    \n",
    "    #Propagate back and add two new columns containing the calculated dr2 position\n",
    "    \n",
    "    z = propagate_back_linear(pairs_list['ra'].to_numpy(), pairs_list['dec'].to_numpy(), pairs_list['pmra'].to_numpy(), pairs_list['pmdec'].to_numpy())\n",
    "    pairs_list['ra_2'] = z.ra.deg\n",
    "    pairs_list['dec_2'] = z.dec.deg\n",
    "    \n",
    "    #List of conditions\n",
    "    conditions = [\n",
    "    (~pairs_list['phot_bp_mean_flux.1'].isna() & ~pairs_list['phot_rp_mean_flux.1'].isna()),\n",
    "    \n",
    "    (pairs_list['phot_bp_mean_flux.1'].isna() & ~pairs_list['phot_rp_mean_flux.1'].isna()),\n",
    "    (~pairs_list['phot_bp_mean_flux.1'].isna() & pairs_list['phot_rp_mean_flux.1'].isna()),\n",
    "    \n",
    "    (pairs_list['phot_bp_mean_flux.1'].isna() & pairs_list['phot_rp_mean_flux.1'].isna()),\n",
    "    ]\n",
    "\n",
    "    ra_offset = (pairs_list['ra_2']-pairs_list['ra.1'])*np.cos(pairs_list['dec_2']*myU.degree)*myU.degree/myU.mas\n",
    "    dec_offset = (pairs_list['dec_2']-pairs_list['dec.1'])*myU.degree/myU.mas\n",
    "    \n",
    "    #Contingent on each condition, evaluate the following normalized norm:\n",
    "    norms = [\n",
    "    (1/7)*(ra_offset**2/(pairs_list['ra_error']**2) + dec_offset**2/(pairs_list['dec_error']**2) + (pairs_list['pmra']-pairs_list['pmra.1'])**2/(pairs_list['pmra_error']**2) + (pairs_list['pmdec']-pairs_list['pmdec.1'])**2/(pairs_list['pmdec_error']**2) + (pairs_list['parallax']-pairs_list['parallax.1'])**2/(pairs_list['parallax_error']**2) + (pairs_list['phot_bp_mean_flux']-pairs_list['phot_bp_mean_flux.1'])**2/(pairs_list['phot_bp_mean_flux_error']**2) + (pairs_list['phot_rp_mean_flux']-pairs_list['phot_rp_mean_flux.1'])**2/(pairs_list['phot_rp_mean_flux_error']**2)),\n",
    "    \n",
    "    (1/6)*(ra_offset**2/(pairs_list['ra_error']**2) + dec_offset**2/(pairs_list['dec_error']**2) + (pairs_list['pmra']-pairs_list['pmra.1'])**2/(pairs_list['pmra_error']**2) + (pairs_list['pmdec']-pairs_list['pmdec.1'])**2/(pairs_list['pmdec_error']**2) + (pairs_list['parallax']-pairs_list['parallax.1'])**2/(pairs_list['parallax_error']**2)  + (pairs_list['phot_rp_mean_flux']-pairs_list['phot_rp_mean_flux.1'])**2/(pairs_list['phot_rp_mean_flux_error']**2)),\n",
    "    (1/6)*(ra_offset**2/(pairs_list['ra_error']**2) + dec_offset**2/(pairs_list['dec_error']**2) + (pairs_list['pmra']-pairs_list['pmra.1'])**2/(pairs_list['pmra_error']**2) + (pairs_list['pmdec']-pairs_list['pmdec.1'])**2/(pairs_list['pmdec_error']**2) + (pairs_list['parallax']-pairs_list['parallax.1'])**2/(pairs_list['parallax_error']**2)  + (pairs_list['phot_bp_mean_flux']-pairs_list['phot_bp_mean_flux.1'])**2/(pairs_list['phot_bp_mean_flux_error']**2)),\n",
    "    \n",
    "    (1/5)*(ra_offset**2/(pairs_list['ra_error']**2) + dec_offset**2/(pairs_list['dec_error']**2) + (pairs_list['pmra']-pairs_list['pmra.1'])**2/(pairs_list['pmra_error']**2) + (pairs_list['pmdec']-pairs_list['pmdec.1'])**2/(pairs_list['pmdec_error']**2) + (pairs_list['parallax']-pairs_list['parallax.1'])**2/(pairs_list['parallax_error']**2)),\n",
    "    \n",
    "    ]\n",
    "\n",
    "    pairs_list['norm'] = np.select(conditions, norms, default=False)\n",
    "    return pairs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_pairs(pairs_list1):\n",
    "    pairs_list = get_norm(pairs_list1)\n",
    "    #mask by condition norm < 4\n",
    "    first_cut = pairs_list[pairs_list['norm']<4]\n",
    "    first_cut.shape\n",
    "    \n",
    "    #Sort by source id, then norm. The duplicates with the smallest norm are at the top of their respective \"chunk.\"\n",
    "    first_cut.sort_values(['source_id', 'norm'], ascending = [True, True],inplace=True)\n",
    "    \n",
    "    #Drop all duplicates, keep the one with the smallest norm\n",
    "    first_cut.drop_duplicates(subset=['source_id'],keep = 'first', inplace=True)\n",
    "    \n",
    "    #Do the same, except for dr2 source. This ensures that two different dr3 sources don't get matched to the same dr2 source.\n",
    "    #Keep the one with the smallest norm\n",
    "    first_cut.sort_values(['source_id.1', 'norm'], ascending = [True, True],inplace=True)\n",
    "    first_cut.drop_duplicates(subset=['source_id.1'],keep = 'first', inplace=True)\n",
    "    \n",
    "    #Re-sort the dataframe by edr3 source id, for convenience\n",
    "    first_cut.sort_values(['source_id', 'norm'], ascending = [True, True],inplace=True)\n",
    "    \n",
    "    return first_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_hacky_accel(th_2, th_3, mu_2, mu_3):\n",
    "    \"\"\"\n",
    "    Function to compute the hacky acceleration vector. \n",
    "    Takes as inputs (N, 2) arrays for the DR2 position, eDR3 position, DR2, proper motion, eDR3 proper motion vectors. \n",
    "    Return (N, 2) array for the acceleration vectors (in mas/y^2).\n",
    "    \"\"\"\n",
    "    t3 = 34.12/12\n",
    "    t2 = 21.96/12\n",
    "\n",
    "    tg3 = 17.26/12\n",
    "    tg2 = 10.6849/12\n",
    "    \n",
    "    \n",
    "    delta_th = np.array([(th_2[:, 0] - th_3[:, 0])*np.cos(th_3[:, 1]*myU.degree)*myU.degree/myU.mas, (th_2[:, 1] - th_3[:, 1])*myU.degree/myU.mas]).T\n",
    "    delta_mu = mu_3*tg3 - mu_2*tg2\n",
    "    \n",
    "    acc_vec = 12*(delta_th + delta_mu)/(t3**2 - t2**2) \n",
    "    \n",
    "    return acc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_accel_cat(dr3, dr2):\n",
    "    #Generate dataframe with dr3 matched with corresponding dr2 source\n",
    "    pairs_list = generate_pairs_list(dr3,dr2)\n",
    "    pair_df1 = get_norm(pairs_list)\n",
    "    pair_df = match_pairs(pair_df1)\n",
    "    \n",
    "    #make (N,2) array of acceleration vectors (mas/y^2)\n",
    "    th_2 = np.array(pair_df[['ra.1', 'dec.1']])\n",
    "    th_3 = np.array(pair_df[['ra', 'dec']])\n",
    "    mu_2 = np.array(pair_df[['pmra.1', 'pmdec.1']])\n",
    "    mu_3 = np.array(pair_df[['pmra', 'pmdec']])\n",
    "\n",
    "    accels = fn_hacky_accel(th_2, th_3, mu_2, mu_3)\n",
    "    \n",
    "    # Anonymous function to find the error of the hacky acceleration\n",
    "    t3 = 34.12/12\n",
    "    t2 = 21.96/12\n",
    "\n",
    "    tg3 = 17.26/12\n",
    "    tg2 = 10.6849/12\n",
    "    \n",
    "    hacky_error = lambda sig_th_2,sig_th_3,sig_mu_2,sig_mu_3 : 12*(np.sqrt(sig_th_2**2 + sig_th_3**2 + tg2**2*sig_mu_2**2 + tg3**2*sig_mu_3**2))/(t3**2 - t2**2)\n",
    "\n",
    "    ra2_error = np.array(pair_df['ra_error.1'])\n",
    "    dec2_error = np.array(pair_df['dec_error.1'])\n",
    "\n",
    "    pmra2_error = np.array(pair_df['pmra_error.1'])\n",
    "    pmdec2_error = np.array(pair_df['pmdec_error.1'])\n",
    "\n",
    "\n",
    "    ra3_error = np.array(pair_df['ra_error'])\n",
    "    dec3_error = np.array(pair_df['dec_error'])\n",
    "\n",
    "    pmra3_error = np.array(pair_df['pmra_error'])\n",
    "    pmdec3_error = np.array(pair_df['pmdec_error'])\n",
    "\n",
    "    # Find acceleration errors\n",
    "    accel_ra_error = hacky_error(ra2_error, ra3_error, pmra2_error, pmra3_error)\n",
    "    accel_dec_error = hacky_error(dec2_error, dec3_error, pmdec2_error, pmdec3_error)    \n",
    "    \n",
    "    pair_df['accel_ra'] = accels[:,0]\n",
    "    pair_df['accel_dec'] = accels[:,1]\n",
    "    \n",
    "    pair_df['accel_ra_error'] = accel_ra_error\n",
    "    pair_df['accel_dec_error']= accel_dec_error\n",
    "    \n",
    "    pair_df.reset_index(inplace = True, drop=True)\n",
    "    # Return minimal acceleration catalogue, with only source ID and accelerations + errors\n",
    "    minimal = pair_df[['source_id','source_id.1','accel_ra', 'accel_ra_error', 'accel_dec', 'accel_dec_error']]\n",
    "    minimal.columns = ['source_id_edr3', 'source_id_dr2','accel_ra', 'accel_ra_error', 'accel_dec', 'accel_dec_error']\n",
    "    return minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Both Catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_cat = generate_pair_cat(dr3, 3, 2) #cutoff at 3 arcsec, 95% CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-547-8c968b655cfc>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_cut.sort_values(['source_id', 'norm'], ascending = [True, True],inplace=True)\n",
      "<ipython-input-547-8c968b655cfc>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_cut.drop_duplicates(subset=['source_id'],keep = 'first', inplace=True)\n",
      "<ipython-input-547-8c968b655cfc>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_cut.sort_values(['source_id.1', 'norm'], ascending = [True, True],inplace=True)\n",
      "<ipython-input-547-8c968b655cfc>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_cut.drop_duplicates(subset=['source_id.1'],keep = 'first', inplace=True)\n",
      "<ipython-input-547-8c968b655cfc>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_cut.sort_values(['source_id', 'norm'], ascending = [True, True],inplace=True)\n",
      "<ipython-input-549-2112352fd918>:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pair_df['accel_ra'] = accels[:,0]\n",
      "<ipython-input-549-2112352fd918>:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pair_df['accel_dec'] = accels[:,1]\n",
      "<ipython-input-549-2112352fd918>:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pair_df['accel_ra_error'] = accel_ra_error\n",
      "<ipython-input-549-2112352fd918>:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pair_df['accel_dec_error']= accel_dec_error\n"
     ]
    }
   ],
   "source": [
    "accel_cat = generate_accel_cat(dr3, dr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When possible, include acceleration of background source in pair_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_accel_cols(pair_cat, accel_cat):\n",
    "    #When possible, this function adds acceleration data to the column in \n",
    "    indices = np.intersect1d(pair_cat['source_id_bg'],accel_cat['source_id_edr3'], return_indices = True)\n",
    "    \n",
    "    accel_ra_to_add = np.empty(len(pair_cat))\n",
    "    accel_dec_to_add = np.empty(len(pair_cat))\n",
    "    \n",
    "    accel_ra_err_to_add = np.empty(len(pair_cat))\n",
    "    accel_dec_err_to_add = np.empty(len(pair_cat))\n",
    "    \n",
    "    #Initialize accelerations with NaN values\n",
    "    accel_ra_to_add[:] = np.nan\n",
    "    accel_dec_to_add[:] = np.nan\n",
    "    \n",
    "    accel_ra_err_to_add[:] = np.nan\n",
    "    accel_dec_err_to_add[:] = np.nan\n",
    "    \n",
    "    for i in range(len(indices[1])):\n",
    "        accel_ra_to_add[indices[1][i]] = accel_cat['accel_ra'].iloc[indices[2][i]]\n",
    "        accel_dec_to_add[indices[1][i]] = accel_cat['accel_dec'].iloc[indices[2][i]]\n",
    "        \n",
    "        accel_ra_err_to_add[indices[1][i]] = accel_cat['accel_ra_error'].iloc[indices[2][i]]\n",
    "        accel_dec_err_to_add[indices[1][i]] = accel_cat['accel_dec_error'].iloc[indices[2][i]]\n",
    "        \n",
    "    pair_cat['accel_ra_bg'] = accel_ra_to_add\n",
    "    pair_cat['accel_ra_error_bg'] = accel_ra_err_to_add\n",
    "    \n",
    "    pair_cat['accel_dec_bg'] = accel_dec_to_add\n",
    "    pair_cat['accel_dec_error_bg'] = accel_dec_err_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_accel_cols(pair_cat, accel_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "accel_cat_name = 'accels_' + str(healpix_edr3_start[current_index]) +'-'+ str(healpix_edr3_end[current_index])\n",
    "accel_cat.to_csv('./acceleration_catalogue/'+ accel_cat_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_cat_name = 'pairs_' + str(healpix_edr3_start[current_index]) +'-'+ str(healpix_edr3_end[current_index])\n",
    "pair_cat.to_csv('./accidental_pairs/' + pair_cat_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
