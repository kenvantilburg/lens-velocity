{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:16:32.868640Z",
     "iopub.status.busy": "2022-01-25T22:16:32.867847Z",
     "iopub.status.idle": "2022-01-25T22:16:34.243908Z",
     "shell.execute_reply": "2022-01-25T22:16:34.243557Z",
     "shell.execute_reply.started": "2022-01-25T22:16:32.868445Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.coordinates import *\n",
    "from astropy.time import Time\n",
    "import astropy.units as u\n",
    "import healpy as hp\n",
    "from scipy.special import erf\n",
    "from scipy import stats\n",
    "import scipy as sp\n",
    "from os import listdir\n",
    "import gzip\n",
    "import sys\n",
    "\n",
    "import myUnitsCopy1 as myU # customized library for units. All dimensional variables are in GeV and GeV=1\n",
    "#import MyUnits as myU # customized library for units. All dimensional variables are in GeV and GeV=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:16:39.576525Z",
     "iopub.status.busy": "2022-01-25T22:16:39.575921Z",
     "iopub.status.idle": "2022-01-25T22:16:39.583862Z",
     "shell.execute_reply": "2022-01-25T22:16:39.582192Z",
     "shell.execute_reply.started": "2022-01-25T22:16:39.576458Z"
    }
   },
   "outputs": [],
   "source": [
    "edr3_data = './edr3_data'\n",
    "dr2_data = './dr2_data'\n",
    "hist_res_dir = './hist_stats/'\n",
    "#edr3_data = '/Users/crimondino/Dropbox (PI)/MyLensVelocity2/data/acc_catalog/edr3/'\n",
    "#dr2_data = '/Users/crimondino/Dropbox (PI)/MyLensVelocity2/data/acc_catalog/dr2/'\n",
    "#hist_res_dir = '/Users/crimondino/Dropbox (PI)/MyLensVelocity2/lists/hist_stats/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:16:48.397731Z",
     "iopub.status.busy": "2022-01-25T22:16:48.397078Z",
     "iopub.status.idle": "2022-01-25T22:16:48.404031Z",
     "shell.execute_reply": "2022-01-25T22:16:48.403288Z",
     "shell.execute_reply.started": "2022-01-25T22:16:48.397663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading in eDR3 file 0.\n"
     ]
    }
   ],
   "source": [
    "### Read in the eDR3 file index from the command line\n",
    "current_index = int(sys.argv[1]) # current index in list of edr3 files\n",
    "print('\\nReading in eDR3 file '+str(current_index)+'.'); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Single EDR3 File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-04T18:15:40.467958Z",
     "iopub.status.busy": "2022-02-04T18:15:40.465956Z",
     "iopub.status.idle": "2022-02-04T18:15:40.484080Z",
     "shell.execute_reply": "2022-02-04T18:15:40.483284Z",
     "shell.execute_reply.started": "2022-02-04T18:15:40.467489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3000/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:16:53.063754Z",
     "iopub.status.busy": "2022-01-25T22:16:53.063112Z",
     "iopub.status.idle": "2022-01-25T22:16:53.071154Z",
     "shell.execute_reply": "2022-01-25T22:16:53.070005Z",
     "shell.execute_reply.started": "2022-01-25T22:16:53.063684Z"
    }
   },
   "outputs": [],
   "source": [
    "list_dr3_files = listdir(edr3_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:16:53.512177Z",
     "iopub.status.busy": "2022-01-25T22:16:53.511828Z",
     "iopub.status.idle": "2022-01-25T22:16:53.523948Z",
     "shell.execute_reply": "2022-01-25T22:16:53.522489Z",
     "shell.execute_reply.started": "2022-01-25T22:16:53.512143Z"
    }
   },
   "outputs": [],
   "source": [
    "healpix_edr3_start = np.empty((len(list_dr3_files)),dtype= int)\n",
    "healpix_edr3_end = np.empty((len(list_dr3_files)), dtype = int)\n",
    "\n",
    "for i,file in enumerate(list_dr3_files):\n",
    "    int_1 = int(file[11:17])\n",
    "    int_2 = int(file[18:24])\n",
    "    healpix_edr3_start[i] = int_1\n",
    "    healpix_edr3_end[i] = int_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:16:53.712036Z",
     "iopub.status.busy": "2022-01-25T22:16:53.711691Z",
     "iopub.status.idle": "2022-01-25T22:16:53.721847Z",
     "shell.execute_reply": "2022-01-25T22:16:53.720729Z",
     "shell.execute_reply.started": "2022-01-25T22:16:53.712002Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_source_ids(file_names):\n",
    "    #given a list of EDR3 filenames, return the start and end source IDs corresponding to healpix level 12\n",
    "    N_8 = 2**(59-16)\n",
    "    \n",
    "    start = np.array([x*N_8 for x in healpix_edr3_start], dtype = 'int')\n",
    "    end = np.array([x*N_8 for x in healpix_edr3_end], dtype = 'int')\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:16:53.900997Z",
     "iopub.status.busy": "2022-01-25T22:16:53.900428Z",
     "iopub.status.idle": "2022-01-25T22:16:53.906270Z",
     "shell.execute_reply": "2022-01-25T22:16:53.905409Z",
     "shell.execute_reply.started": "2022-01-25T22:16:53.900932Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dr3_file(idx):\n",
    "    return pd.read_csv(edr3_data + '/' + list_dr3_files[idx], compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:16:54.053918Z",
     "iopub.status.busy": "2022-01-25T22:16:54.053594Z",
     "iopub.status.idle": "2022-01-25T22:16:54.057756Z",
     "shell.execute_reply": "2022-01-25T22:16:54.056863Z",
     "shell.execute_reply.started": "2022-01-25T22:16:54.053886Z"
    }
   },
   "outputs": [],
   "source": [
    "start, end = get_source_ids(list_dr3_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Corresponding DR2 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:16:54.446133Z",
     "iopub.status.busy": "2022-01-25T22:16:54.445792Z",
     "iopub.status.idle": "2022-01-25T22:16:54.453658Z",
     "shell.execute_reply": "2022-01-25T22:16:54.452676Z",
     "shell.execute_reply.started": "2022-01-25T22:16:54.446099Z"
    }
   },
   "outputs": [],
   "source": [
    "list_dr2_files = np.array([file for file in listdir(dr2_data) if file[-7:]=='.csv.gz']) #select only files ending with 'csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:16:54.632445Z",
     "iopub.status.busy": "2022-01-25T22:16:54.632129Z",
     "iopub.status.idle": "2022-01-25T22:16:54.643383Z",
     "shell.execute_reply": "2022-01-25T22:16:54.642148Z",
     "shell.execute_reply.started": "2022-01-25T22:16:54.632411Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dr2_files(idx):\n",
    "    #separate into two arrays of start/end source IDs\n",
    "    strings = np.array([file.split('_') for file in list_dr2_files])\n",
    "    sid_dr2_start = np.array([int(name) for name in strings[:,1]])\n",
    "    sid_dr2_end = np.array([int(name[:-7]) for name in strings[:,2]])\n",
    "\n",
    "    pass1 = np.where(start[idx] < sid_dr2_end)[0]\n",
    "    pass2 = np.where(end[idx] < sid_dr2_start)[0]\n",
    "\n",
    "    file_indices = np.setdiff1d(pass1, pass2)\n",
    "\n",
    "    files_to_open = list_dr2_files[file_indices]\n",
    "    print(str(len(files_to_open))+ ' corresponding files')\n",
    "    return pd.concat((pd.read_csv(dr2_data+ '/' + str(f), compression = 'gzip') for f in files_to_open))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:16:55.048954Z",
     "iopub.status.busy": "2022-01-25T22:16:55.048592Z",
     "iopub.status.idle": "2022-01-25T22:17:09.650270Z",
     "shell.execute_reply": "2022-01-25T22:17:09.649518Z",
     "shell.execute_reply.started": "2022-01-25T22:16:55.048916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 corresponding files\n"
     ]
    }
   ],
   "source": [
    "dr3 = load_dr3_file(current_index)\n",
    "dr2 = load_dr2_files(current_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Pair Catalogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each star, we first locate accidental pairs by on-sky proximity. This is the condition\n",
    "$$|\\theta_i - \\theta_j| < \\theta_\\text{min}$$\n",
    "where $i$ is the index of the foreground star, and $j$ is a background star for a given foreground $i$.\n",
    "After this first cut, we then impose that the background candidate be behind the foreground at $n_\\sigma$. \n",
    "\n",
    "$$\\varpi_i - \\varpi_j > n_\\sigma \\sqrt{\\sigma_{\\varpi_i}^2 + \\sigma_{\\varpi_j}^2}.$$\n",
    "\n",
    "When $n_\\sigma =2$, this corresponds to a 95% confidence level. We can tighten or relax these cuts in order to control the size/purity of the resulting pair catalogue.\n",
    "\n",
    "The above assumes that $\\sigma_{\\varpi_i}$ and $\\sigma_{\\varpi_j}$ have zero correlation. A stricter condition would be to assume that they had correlation = 1. If so, then the above formula becomes \n",
    "$$\\varpi_i - n_\\sigma \\sigma_{\\varpi_i} > \\varpi_i + n_\\sigma \\sigma_{\\varpi_j}.$$\n",
    "This results in fewer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:09.651627Z",
     "iopub.status.busy": "2022-01-25T22:17:09.651507Z",
     "iopub.status.idle": "2022-01-25T22:17:09.657272Z",
     "shell.execute_reply": "2022-01-25T22:17:09.656912Z",
     "shell.execute_reply.started": "2022-01-25T22:17:09.651615Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_pair_cat(df, angle_cutoff, n_sigma):\n",
    "    #Note that angle_cutoff is measured in arcseconds.\n",
    "    \n",
    "    ra_arr = np.asarray(df['ra'])\n",
    "    dec_arr = np.asarray(df['dec'])\n",
    "    coord1 = SkyCoord(ra_arr, dec_arr, unit = u.degree)\n",
    "    \n",
    "    #Search df for on-sky neighbors within angle_cutoff arsec\n",
    "    z = search_around_sky(coord1, coord1, angle_cutoff*u.arcsec, storekdtree = False)\n",
    "    \n",
    "    #The above snippet will count a foreground star as its own neighbor, so we must remove them:\n",
    "    idx = z[0][z[0] != z[1]]\n",
    "    dub = z[1][z[0] != z[1]]\n",
    "    \n",
    "    df_fore = df.iloc[idx]\n",
    "    df_back = df.iloc[dub]\n",
    "    \n",
    "    df_fore.reset_index(inplace = True, drop=True)\n",
    "    df_back.reset_index(inplace = True, drop=True)\n",
    "    \n",
    "    #Define a function to iterate over the foreground/background df's and check if they satisfy the parallax condition\n",
    "\n",
    "    is_behind = lambda par1, par2, err1, err2 : par1-par2 > n_sigma*np.sqrt(err1**2 + err2**2)\n",
    "    is_behind_list = is_behind(df_fore['parallax'], df_back['parallax'], df_fore['parallax_error'], df_back['parallax_error'])\n",
    "    \n",
    "    #Keep pairs that satisfy the parallax condition within n_sigma. \n",
    "    df_fore = df_fore[is_behind_list]\n",
    "    df_back = df_back[is_behind_list]\n",
    "    \n",
    "    #Concatenate the foreground and background list into one catalogue.\n",
    "    new_fg_cols = [x+\"_fg\" for x in df_fore.columns]\n",
    "    df_fore.columns= new_fg_cols\n",
    "    \n",
    "    new_bg_cols = [x+\"_bg\" for x in df_back.columns]\n",
    "    df_back.columns= new_bg_cols\n",
    "    \n",
    "    pair_cat = pd.concat([df_fore,df_back], axis = 1)\n",
    "    pair_cat.reset_index(inplace =True, drop = True)\n",
    "    return pair_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Acceleration Catalogue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:09.658409Z",
     "iopub.status.busy": "2022-01-25T22:17:09.658322Z",
     "iopub.status.idle": "2022-01-25T22:17:09.662992Z",
     "shell.execute_reply": "2022-01-25T22:17:09.662037Z",
     "shell.execute_reply.started": "2022-01-25T22:17:09.658398Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_pairs_list(dr3, dr2):\n",
    "    ra_arr1 = np.asarray(dr3['ra'])\n",
    "    dec_arr1 = np.asarray(dr3['dec'])\n",
    "\n",
    "    ra_arr2 = np.asarray(dr2['ra'])\n",
    "    dec_arr2 = np.asarray(dr2['dec'])\n",
    "    \n",
    "    coord1 = SkyCoord(ra_arr1, dec_arr1, unit = u.degree)\n",
    "    coord2 = SkyCoord(ra_arr2, dec_arr2, unit = u.degree)\n",
    "    \n",
    "    z = search_around_sky(coord1, coord2, 3*u.arcsec, storekdtree = False)\n",
    "    \n",
    "    df1 = dr3.iloc[z[0]]\n",
    "    df2 = dr2.iloc[z[1]]\n",
    "    \n",
    "    df1.reset_index(inplace = True, drop=True)\n",
    "    df2.reset_index(inplace = True, drop=True)\n",
    "    \n",
    "    new_cols = [x+\".1\" for x in df2.columns]\n",
    "    df2.columns= new_cols\n",
    "    result = pd.concat([df1,df2], axis = 1)\n",
    "    result = result[(result['astrometric_params_solved']>= 27) & (result['astrometric_params_solved.1']>= 27)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:09.665401Z",
     "iopub.status.busy": "2022-01-25T22:17:09.665249Z",
     "iopub.status.idle": "2022-01-25T22:17:09.669549Z",
     "shell.execute_reply": "2022-01-25T22:17:09.668953Z",
     "shell.execute_reply.started": "2022-01-25T22:17:09.665387Z"
    }
   },
   "outputs": [],
   "source": [
    "def propagate_back_linear(ra_g3, dec_g3, pmra_g3, pmdec_g3):\n",
    "    \"\"\"Takes EDR3 position and proper motion, and linearly propagates it by 0.5 year to the DR2 epoch. Output: SkyCoord object in DR2 epoch. Does not take into account parallax.\"\"\"\n",
    "    c = SkyCoord(ra = ra_g3 * u.deg, \n",
    "                 dec = dec_g3 * u.deg, \n",
    "                 distance = 1 * u.kpc, #setting distance to 1 kpc, otherwise it thinks stuff is at 10 Mpc and then returns an exception due to faster than light\n",
    "                 pm_ra_cosdec = pmra_g3 * u.mas/u.yr,\n",
    "                 pm_dec = pmdec_g3 * u.mas/u.yr,\n",
    "                 obstime = Time(2016.0, format='jyear'))\n",
    "    return c.apply_space_motion(Time(2015.5, format='jyear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:09.670396Z",
     "iopub.status.busy": "2022-01-25T22:17:09.670289Z",
     "iopub.status.idle": "2022-01-25T22:17:09.681504Z",
     "shell.execute_reply": "2022-01-25T22:17:09.681152Z",
     "shell.execute_reply.started": "2022-01-25T22:17:09.670384Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_norm(pairs_list):\n",
    "    \n",
    "    #Propagate back and add two new columns containing the calculated dr2 position\n",
    "    \n",
    "    z = propagate_back_linear(pairs_list['ra'].to_numpy(), pairs_list['dec'].to_numpy(), pairs_list['pmra'].to_numpy(), pairs_list['pmdec'].to_numpy())\n",
    "    pairs_list['ra_2'] = z.ra.deg\n",
    "    pairs_list['dec_2'] = z.dec.deg\n",
    "    \n",
    "    #List of conditions\n",
    "    conditions = [\n",
    "    (~pairs_list['phot_bp_mean_flux.1'].isna() & ~pairs_list['phot_rp_mean_flux.1'].isna()),\n",
    "    \n",
    "    (pairs_list['phot_bp_mean_flux.1'].isna() & ~pairs_list['phot_rp_mean_flux.1'].isna()),\n",
    "    (~pairs_list['phot_bp_mean_flux.1'].isna() & pairs_list['phot_rp_mean_flux.1'].isna()),\n",
    "    \n",
    "    (pairs_list['phot_bp_mean_flux.1'].isna() & pairs_list['phot_rp_mean_flux.1'].isna()),\n",
    "    ]\n",
    "\n",
    "    ra_offset = (pairs_list['ra_2']-pairs_list['ra.1'])*np.cos(pairs_list['dec_2']*myU.degree)*myU.degree/myU.mas\n",
    "    dec_offset = (pairs_list['dec_2']-pairs_list['dec.1'])*myU.degree/myU.mas\n",
    "    \n",
    "    #Contingent on each condition, evaluate the following normalized norm:\n",
    "    norms = [\n",
    "    (1/7)*(ra_offset**2/(pairs_list['ra_error']**2) + dec_offset**2/(pairs_list['dec_error']**2) + (pairs_list['pmra']-pairs_list['pmra.1'])**2/(pairs_list['pmra_error']**2) + (pairs_list['pmdec']-pairs_list['pmdec.1'])**2/(pairs_list['pmdec_error']**2) + (pairs_list['parallax']-pairs_list['parallax.1'])**2/(pairs_list['parallax_error']**2) + (pairs_list['phot_bp_mean_flux']-pairs_list['phot_bp_mean_flux.1'])**2/(pairs_list['phot_bp_mean_flux_error']**2) + (pairs_list['phot_rp_mean_flux']-pairs_list['phot_rp_mean_flux.1'])**2/(pairs_list['phot_rp_mean_flux_error']**2)),\n",
    "    \n",
    "    (1/6)*(ra_offset**2/(pairs_list['ra_error']**2) + dec_offset**2/(pairs_list['dec_error']**2) + (pairs_list['pmra']-pairs_list['pmra.1'])**2/(pairs_list['pmra_error']**2) + (pairs_list['pmdec']-pairs_list['pmdec.1'])**2/(pairs_list['pmdec_error']**2) + (pairs_list['parallax']-pairs_list['parallax.1'])**2/(pairs_list['parallax_error']**2)  + (pairs_list['phot_rp_mean_flux']-pairs_list['phot_rp_mean_flux.1'])**2/(pairs_list['phot_rp_mean_flux_error']**2)),\n",
    "    (1/6)*(ra_offset**2/(pairs_list['ra_error']**2) + dec_offset**2/(pairs_list['dec_error']**2) + (pairs_list['pmra']-pairs_list['pmra.1'])**2/(pairs_list['pmra_error']**2) + (pairs_list['pmdec']-pairs_list['pmdec.1'])**2/(pairs_list['pmdec_error']**2) + (pairs_list['parallax']-pairs_list['parallax.1'])**2/(pairs_list['parallax_error']**2)  + (pairs_list['phot_bp_mean_flux']-pairs_list['phot_bp_mean_flux.1'])**2/(pairs_list['phot_bp_mean_flux_error']**2)),\n",
    "    \n",
    "    (1/5)*(ra_offset**2/(pairs_list['ra_error']**2) + dec_offset**2/(pairs_list['dec_error']**2) + (pairs_list['pmra']-pairs_list['pmra.1'])**2/(pairs_list['pmra_error']**2) + (pairs_list['pmdec']-pairs_list['pmdec.1'])**2/(pairs_list['pmdec_error']**2) + (pairs_list['parallax']-pairs_list['parallax.1'])**2/(pairs_list['parallax_error']**2)),\n",
    "    \n",
    "    ]\n",
    "\n",
    "    pairs_list['norm'] = np.select(conditions, norms, default=False)\n",
    "    return pairs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:09.682281Z",
     "iopub.status.busy": "2022-01-25T22:17:09.682185Z",
     "iopub.status.idle": "2022-01-25T22:17:09.686420Z",
     "shell.execute_reply": "2022-01-25T22:17:09.685834Z",
     "shell.execute_reply.started": "2022-01-25T22:17:09.682270Z"
    }
   },
   "outputs": [],
   "source": [
    "def match_pairs(pairs_list1):\n",
    "    pairs_list = get_norm(pairs_list1)\n",
    "    #mask by condition norm < 4\n",
    "    first_cut = pairs_list[pairs_list['norm']<4]\n",
    "    first_cut.shape\n",
    "    \n",
    "    #Sort by source id, then norm. The duplicates with the smallest norm are at the top of their respective \"chunk.\"\n",
    "    first_cut.sort_values(['source_id', 'norm'], ascending = [True, True],inplace=True)\n",
    "    \n",
    "    #Drop all duplicates, keep the one with the smallest norm\n",
    "    first_cut.drop_duplicates(subset=['source_id'],keep = 'first', inplace=True)\n",
    "    \n",
    "    #Do the same, except for dr2 source. This ensures that two different dr3 sources don't get matched to the same dr2 source.\n",
    "    #Keep the one with the smallest norm\n",
    "    first_cut.sort_values(['source_id.1', 'norm'], ascending = [True, True],inplace=True)\n",
    "    first_cut.drop_duplicates(subset=['source_id.1'],keep = 'first', inplace=True)\n",
    "    \n",
    "    #Re-sort the dataframe by edr3 source id, for convenience\n",
    "    first_cut.sort_values(['source_id', 'norm'], ascending = [True, True],inplace=True)\n",
    "    \n",
    "    return first_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:09.687324Z",
     "iopub.status.busy": "2022-01-25T22:17:09.687219Z",
     "iopub.status.idle": "2022-01-25T22:17:09.692039Z",
     "shell.execute_reply": "2022-01-25T22:17:09.691571Z",
     "shell.execute_reply.started": "2022-01-25T22:17:09.687311Z"
    }
   },
   "outputs": [],
   "source": [
    "def fn_hacky_accel(th_2, th_3, mu_2, mu_3):\n",
    "    \"\"\"\n",
    "    Function to compute the hacky acceleration vector. \n",
    "    Takes as inputs (N, 2) arrays for the DR2 position, eDR3 position, DR2, proper motion, eDR3 proper motion vectors. \n",
    "    Return (N, 2) array for the acceleration vectors (in mas/y^2).\n",
    "    \"\"\"\n",
    "    t3 = 34.12/12\n",
    "    t2 = 21.96/12\n",
    "\n",
    "    tg3 = 17.26/12\n",
    "    tg2 = 10.6849/12\n",
    "    \n",
    "    \n",
    "    delta_th = np.array([(th_2[:, 0] - th_3[:, 0])*np.cos(th_3[:, 1]*myU.degree)*myU.degree/myU.mas, (th_2[:, 1] - th_3[:, 1])*myU.degree/myU.mas]).T\n",
    "    delta_mu = mu_3*tg3 - mu_2*tg2\n",
    "    \n",
    "    acc_vec = 12*(delta_th + delta_mu)/(t3**2 - t2**2) \n",
    "    \n",
    "    return acc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:09.694522Z",
     "iopub.status.busy": "2022-01-25T22:17:09.694356Z",
     "iopub.status.idle": "2022-01-25T22:17:09.702028Z",
     "shell.execute_reply": "2022-01-25T22:17:09.701525Z",
     "shell.execute_reply.started": "2022-01-25T22:17:09.694504Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_accel_cat(dr3, dr2):\n",
    "    #Generate dataframe with dr3 matched with corresponding dr2 source\n",
    "    pairs_list = generate_pairs_list(dr3,dr2)\n",
    "    pair_df1 = get_norm(pairs_list)\n",
    "    pair_df = match_pairs(pair_df1)\n",
    "    \n",
    "    #make (N,2) array of acceleration vectors (mas/y^2)\n",
    "    th_2 = np.array(pair_df[['ra.1', 'dec.1']])\n",
    "    th_3 = np.array(pair_df[['ra', 'dec']])\n",
    "    mu_2 = np.array(pair_df[['pmra.1', 'pmdec.1']])\n",
    "    mu_3 = np.array(pair_df[['pmra', 'pmdec']])\n",
    "\n",
    "    accels = fn_hacky_accel(th_2, th_3, mu_2, mu_3)\n",
    "    \n",
    "    # Anonymous function to find the error of the hacky acceleration\n",
    "    t3 = 34.12/12\n",
    "    t2 = 21.96/12\n",
    "\n",
    "    tg3 = 17.26/12\n",
    "    tg2 = 10.6849/12\n",
    "    \n",
    "    hacky_error = lambda sig_th_2,sig_th_3,sig_mu_2,sig_mu_3 : 12*(np.sqrt(sig_th_2**2 + sig_th_3**2 + tg2**2*sig_mu_2**2 + tg3**2*sig_mu_3**2))/(t3**2 - t2**2)\n",
    "\n",
    "    ra2_error = np.array(pair_df['ra_error.1'])\n",
    "    dec2_error = np.array(pair_df['dec_error.1'])\n",
    "\n",
    "    pmra2_error = np.array(pair_df['pmra_error.1'])\n",
    "    pmdec2_error = np.array(pair_df['pmdec_error.1'])\n",
    "\n",
    "\n",
    "    ra3_error = np.array(pair_df['ra_error'])\n",
    "    dec3_error = np.array(pair_df['dec_error'])\n",
    "\n",
    "    pmra3_error = np.array(pair_df['pmra_error'])\n",
    "    pmdec3_error = np.array(pair_df['pmdec_error'])\n",
    "\n",
    "    # Find acceleration errors\n",
    "    accel_ra_error = hacky_error(ra2_error, ra3_error, pmra2_error, pmra3_error)\n",
    "    accel_dec_error = hacky_error(dec2_error, dec3_error, pmdec2_error, pmdec3_error)    \n",
    "    \n",
    "    pair_df['accel_ra'] = accels[:,0]\n",
    "    pair_df['accel_dec'] = accels[:,1]\n",
    "    \n",
    "    pair_df['accel_ra_error'] = accel_ra_error\n",
    "    pair_df['accel_dec_error']= accel_dec_error\n",
    "    \n",
    "    pair_df.reset_index(inplace = True, drop=True)\n",
    "    # Return minimal acceleration catalogue, with only source ID and accelerations + errors\n",
    "    minimal = pair_df[['source_id','source_id.1','accel_ra', 'accel_ra_error', 'accel_dec', 'accel_dec_error']]\n",
    "    minimal.columns = ['source_id_edr3', 'source_id_dr2','accel_ra', 'accel_ra_error', 'accel_dec', 'accel_dec_error']\n",
    "    \n",
    "    acc_stats_col = ['source_id', 'parallax', 'parallax_error', 'accel_ra', 'accel_dec', \n",
    "                     'accel_ra_error', 'accel_dec_error', 'phot_g_mean_mag']\n",
    "    \n",
    "    return minimal, pair_df[acc_stats_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acceleration statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:09.702993Z",
     "iopub.status.busy": "2022-01-25T22:17:09.702885Z",
     "iopub.status.idle": "2022-01-25T22:17:09.708305Z",
     "shell.execute_reply": "2022-01-25T22:17:09.707226Z",
     "shell.execute_reply.started": "2022-01-25T22:17:09.702979Z"
    }
   },
   "outputs": [],
   "source": [
    "nside = 2**8\n",
    "fac_source_id = 2**(59-2*8)\n",
    "npix = hp.nside2npix(nside)\n",
    "#print('nside =',nside,', npix =',npix)\n",
    "#print('linear pixel size =',str(np.sqrt(4*np.pi / npix) / arcsec)[0:7],' arcsec =', str(np.sqrt(4*np.pi / npix) / degree)[0:7],' degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:09.709168Z",
     "iopub.status.busy": "2022-01-25T22:17:09.709009Z",
     "iopub.status.idle": "2022-01-25T22:17:09.716137Z",
     "shell.execute_reply": "2022-01-25T22:17:09.715765Z",
     "shell.execute_reply.started": "2022-01-25T22:17:09.709153Z"
    }
   },
   "outputs": [],
   "source": [
    "# bin definitions\n",
    "bins_parallax = np.concatenate([[-1000],np.logspace(np.log10(0.05),np.log10(2),10),[1000]])\n",
    "#print(bins_parallax)\n",
    "bins_G = np.arange(3,23,1) # floor or the min and max G mag in the entire catalog are 3 and 21\n",
    "#print(bins_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:09.716794Z",
     "iopub.status.busy": "2022-01-25T22:17:09.716692Z",
     "iopub.status.idle": "2022-01-25T22:17:09.742949Z",
     "shell.execute_reply": "2022-01-25T22:17:09.741255Z",
     "shell.execute_reply.started": "2022-01-25T22:17:09.716782Z"
    }
   },
   "outputs": [],
   "source": [
    "def fn_acc_stats(tab, th_count=3, return_tab=False, n_sigma_out = 3): \n",
    "    \"\"\"\n",
    "    Bins the stars in tab in healpix, G mag and parallax and computes the mean and variance of acc_ra and acc_dec per bin.\n",
    "    If return_tab=False, returns the statistic in each bin.\n",
    "    If return_tab=True, returns the stars in tab removing the outliers at more than n_sigma_out from the mean.\n",
    "    \"\"\"\n",
    "\n",
    "    ### healpix binning\n",
    "    q_pix = np.floor(tab['source_id'].to_numpy() / fac_source_id).astype(int)\n",
    "    bins_pix = np.arange(np.min(np.unique(q_pix)), np.max(np.unique(q_pix))+2,1) # should be +2 to include sources in the last bin\n",
    "    q_binpix = np.digitize(q_pix, bins_pix)-1  # need to access the histogram matrix elements\n",
    "\n",
    "    ### assign to G bins\n",
    "    tab_G = tab['phot_g_mean_mag'].to_numpy()\n",
    "    q_binG = np.digitize(tab_G, bins_G)-1      \n",
    "    \n",
    "    ### probabilistic assignment to parallax bins\n",
    "    tab_parallax = tab['parallax'].to_numpy(); tab_parallax_error = tab['parallax_error'].to_numpy();\n",
    "    prob_parallax = np.nan * np.ones((len(tab),len(bins_parallax)-1))\n",
    "    for i in range(len(bins_parallax)-1):\n",
    "        x1_list = (bins_parallax[i]-tab_parallax)/tab_parallax_error/np.sqrt(2)\n",
    "        x2_list = (bins_parallax[i+1]-tab_parallax)/tab_parallax_error/np.sqrt(2)\n",
    "        prob_parallax[:,i] = 0.5*(erf(x2_list)-erf(x1_list))\n",
    "\n",
    "    tab_acc_ra = tab['accel_ra'].to_numpy(); tab_acc_dec = tab['accel_dec'].to_numpy();\n",
    "    ### histogram of summed probabilities\n",
    "    hist_prob = stats.binned_statistic_dd([tab_G,q_pix],np.transpose(prob_parallax), bins=[bins_G,bins_pix],statistic='sum')[0] \n",
    "    ### histogram of average acc_ra weighted by probabilities\n",
    "    hist_acc_ra = stats.binned_statistic_dd([tab_G,q_pix],np.transpose(prob_parallax) * tab_acc_ra, bins=[bins_G,bins_pix],statistic='sum')[0] #sum first in each bin\n",
    "    hist_acc_ra = hist_acc_ra / (hist_prob + 1e-20) #then divide by number in each bin\n",
    "    hist_acc_dec = stats.binned_statistic_dd([tab_G,q_pix],np.transpose(prob_parallax) * tab_acc_dec, bins=[bins_G,bins_pix],statistic='sum')[0] #sum first in each bin\n",
    "    hist_acc_dec = hist_acc_dec / (hist_prob + 1e-20) #then divide by number in each bin\n",
    "    \n",
    "    ### For each star, get the mean acc of the corresponding bin\n",
    "    mean_acc_ra = hist_acc_ra[:, q_binG, q_binpix].T; mean_acc_dec = hist_acc_dec[:, q_binG, q_binpix].T\n",
    "\n",
    "    ### histogram of acc variance weighted by parallax bin probabilities\n",
    "    hist_acc_ra_var = stats.binned_statistic_dd([tab_G,q_pix],np.transpose(prob_parallax) * (mean_acc_ra.T - tab_acc_ra)**2,\n",
    "                                                   bins=[bins_G,bins_pix],statistic='sum')[0] #sum first in each bin\n",
    "    hist_acc_ra_var = hist_acc_ra_var / (hist_prob - 1 + 1e-20) # the estimator should have a -1 (this matches for example var() computed with panda's groupy)\n",
    "    hist_acc_dec_var = stats.binned_statistic_dd([tab_G,q_pix],np.transpose(prob_parallax) * (mean_acc_dec.T - tab_acc_dec)**2,\n",
    "                                                    bins=[bins_G,bins_pix],statistic='sum')[0] #sum first in each bin\n",
    "    hist_acc_dec_var = hist_acc_dec_var / (hist_prob - 1 + 1e-20) \n",
    "    hist_acc_radec_var = stats.binned_statistic_dd([tab_G,q_pix],np.transpose(prob_parallax) * (mean_acc_ra.T - tab_acc_ra) * (mean_acc_dec.T - tab_acc_dec),\n",
    "                                                      bins=[bins_G,bins_pix],statistic='sum')[0] #sum first in each bin\n",
    "    hist_acc_radec_var = hist_acc_radec_var / (hist_prob - 1 + 1e-20) \n",
    "    \n",
    "    ### set to nan bins where there are too few stars\n",
    "    hist_acc_ra[hist_prob < th_count] = np.nan; hist_acc_dec[hist_prob < th_count] = np.nan\n",
    "    hist_acc_ra_var[hist_prob < th_count] = np.nan; hist_acc_dec_var[hist_prob < th_count] = np.nan; hist_acc_radec_var[hist_prob < th_count] = np.nan\n",
    "\n",
    "    if return_tab==False: # returns the data frame with the statistics computed using tab\n",
    "        ###  filler for generalized bins indices\n",
    "        hist_bins_pix = np.ones(np.shape(hist_prob)) * bins_pix[:-1]\n",
    "        hist_bins_G = np.transpose(np.transpose(np.ones(np.shape(hist_prob)),axes=[0,2,1]) * bins_G[:-1],axes=[0,2,1])\n",
    "        hist_bins_parallax = np.transpose(np.transpose(np.ones(np.shape(hist_prob)),axes=[2,1,0]) * bins_parallax[:-1],axes=[2,1,0])\n",
    "\n",
    "        ###  collect data and output\n",
    "        data = np.transpose([hist_bins_pix, hist_bins_G, hist_bins_parallax, hist_prob, hist_acc_ra, hist_acc_dec, hist_acc_ra_var, hist_acc_dec_var, hist_acc_radec_var],axes=[1,2,3,0])\n",
    "        data = data.reshape(-1, data.shape[-1])\n",
    "        return pd.DataFrame(data,columns=['pix','G_bin','parallax_bin','number','mean_acc_ra','mean_acc_dec','var_acc_ra','var_acc_dec','var_acc_radec'])\n",
    "    \n",
    "    else: # returns tab where the acc outliers more than n_sigma_out away from zero have been removed\n",
    "        ### For each star, get the acc mean and variance of the corresponding bin (after excluding the low count bins)\n",
    "        mean_acc_ra = hist_acc_ra[:, q_binG, q_binpix].T; mean_acc_dec = hist_acc_dec[:, q_binG, q_binpix].T\n",
    "        var_acc_ra = hist_acc_ra_var[:, q_binG, q_binpix].T; var_acc_dec = hist_acc_dec_var[:, q_binG, q_binpix].T; var_acc_radec = hist_acc_radec_var[:, q_binG, q_binpix].T;    \n",
    "\n",
    "        ###  Get the mean and var for each star\n",
    "        tab_sum_pw = np.sum(prob_parallax, axis=1, where=(~np.isnan(mean_acc_ra)))  # sum of the parallax weights for each star using only bins with enough statistics \n",
    "        tab_mean_acc_ra = np.sum(np.nan_to_num(mean_acc_ra*prob_parallax), axis=1)/(tab_sum_pw + 1e-20)\n",
    "        tab_mean_acc_dec = np.sum(np.nan_to_num(mean_acc_dec*prob_parallax), axis=1)/(tab_sum_pw + 1e-20)\n",
    "        tab_var_acc_ra = np.sum(np.nan_to_num(var_acc_ra*prob_parallax), axis=1)/(tab_sum_pw + 1e-20)\n",
    "        tab_var_acc_dec = np.sum(np.nan_to_num(var_acc_dec*prob_parallax), axis=1)/(tab_sum_pw + 1e-20)\n",
    "        tab_var_acc_radec = np.sum(np.nan_to_num(var_acc_radec*prob_parallax), axis=1)/(tab_sum_pw + 1e-20)        \n",
    "        \n",
    "        ### Replace the effective variance with the measurement errors for stars that have 0 mean (fall into empty bins)\n",
    "        tab_var_acc_ra[tab_var_acc_ra==0] = (tab['accel_ra_error'].to_numpy()[tab_var_acc_ra==0])**2\n",
    "        tab_var_acc_dec[tab_var_acc_dec==0] = (tab['accel_dec_error'].to_numpy()[tab_var_acc_dec==0])**2\n",
    "        tab_var_acc_radec[tab_var_acc_radec==0] = (np.zeros(len(tab))*tab['accel_ra_error'].to_numpy()*tab['accel_dec_error'].to_numpy())[tab_var_acc_radec==0]\n",
    "        \n",
    "        ### subtracted acc and inverse covariance for outlier removal\n",
    "        acc_sub = np.array([tab['accel_ra'].to_numpy()-tab_mean_acc_ra, tab['accel_dec'].to_numpy()-tab_mean_acc_dec]).T\n",
    "        inv_cov_acc = np.linalg.inv(np.array([[tab_var_acc_ra, tab_var_acc_radec], [tab_var_acc_radec, tab_var_acc_dec]]).T)\n",
    "        acc_over_sigma_sq = inv_cov_acc[:, 0, 0]*acc_sub[:, 0]**2 + inv_cov_acc[:, 1, 1]*acc_sub[:, 1]**2 + 2*inv_cov_acc[:, 0, 1]*acc_sub[:, 0]*acc_sub[:, 1]\n",
    "        \n",
    "        return tab.iloc[acc_over_sigma_sq < n_sigma_out**2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:09.744429Z",
     "iopub.status.busy": "2022-01-25T22:17:09.744310Z",
     "iopub.status.idle": "2022-01-25T22:17:09.750353Z",
     "shell.execute_reply": "2022-01-25T22:17:09.749403Z",
     "shell.execute_reply.started": "2022-01-25T22:17:09.744415Z"
    }
   },
   "outputs": [],
   "source": [
    "def fn_execute(tab, i_f, n_iter=10, flag_print=False):\n",
    "        \n",
    "    tab = tab.loc[tab['phot_g_mean_mag']>0] # only look at stars with pm and G > 0\n",
    "\n",
    "    i=0; out_frac=1\n",
    "    while (i<n_iter) & (out_frac>1E-5):\n",
    "        tab_temp = fn_acc_stats(tab, th_count=3, return_tab=True, n_sigma_out = 3)\n",
    "        i+=1; out_frac=(1-len(tab_temp)/len(tab)); \n",
    "        if flag_print==True:\n",
    "            print('Iter '+str(i)+' -- fraction of outliers removed: '+str(out_frac*100)[:7]+' %'); sys.stdout.flush()\n",
    "        tab = tab_temp \n",
    "        \n",
    "    df_acc_stats = fn_acc_stats(tab, return_tab=False) \n",
    "    df_acc_stats.bins_parallax(hist_res_dir+list_dr3_files[i_f][:-7]+'_hist.csv', index=False) #write to file\n",
    "        \n",
    "    return len(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Both Catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:09.751686Z",
     "iopub.status.busy": "2022-01-25T22:17:09.751466Z",
     "iopub.status.idle": "2022-01-25T22:17:11.451739Z",
     "shell.execute_reply": "2022-01-25T22:17:11.451382Z",
     "shell.execute_reply.started": "2022-01-25T22:17:09.751668Z"
    }
   },
   "outputs": [],
   "source": [
    "pair_cat = generate_pair_cat(dr3, 3, 2) #cutoff at 3 arcsec, 95% CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:12.128909Z",
     "iopub.status.busy": "2022-01-25T22:17:12.128285Z",
     "iopub.status.idle": "2022-01-25T22:17:21.202662Z",
     "shell.execute_reply": "2022-01-25T22:17:21.202238Z",
     "shell.execute_reply.started": "2022-01-25T22:17:12.128842Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-8c968b655cfc>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_cut.sort_values(['source_id', 'norm'], ascending = [True, True],inplace=True)\n",
      "<ipython-input-18-8c968b655cfc>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_cut.drop_duplicates(subset=['source_id'],keep = 'first', inplace=True)\n",
      "<ipython-input-18-8c968b655cfc>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_cut.sort_values(['source_id.1', 'norm'], ascending = [True, True],inplace=True)\n",
      "<ipython-input-18-8c968b655cfc>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_cut.drop_duplicates(subset=['source_id.1'],keep = 'first', inplace=True)\n",
      "<ipython-input-18-8c968b655cfc>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_cut.sort_values(['source_id', 'norm'], ascending = [True, True],inplace=True)\n",
      "<ipython-input-20-26fcecd75fb1>:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pair_df['accel_ra'] = accels[:,0]\n",
      "<ipython-input-20-26fcecd75fb1>:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pair_df['accel_dec'] = accels[:,1]\n",
      "<ipython-input-20-26fcecd75fb1>:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pair_df['accel_ra_error'] = accel_ra_error\n",
      "<ipython-input-20-26fcecd75fb1>:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pair_df['accel_dec_error']= accel_dec_error\n"
     ]
    }
   ],
   "source": [
    "accel_cat, tab_acc_stat = generate_accel_cat(dr3, dr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When possible, include acceleration of background source in pair_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:30.267419Z",
     "iopub.status.busy": "2022-01-25T22:17:30.266962Z",
     "iopub.status.idle": "2022-01-25T22:17:30.284866Z",
     "shell.execute_reply": "2022-01-25T22:17:30.283806Z",
     "shell.execute_reply.started": "2022-01-25T22:17:30.267381Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_accel_cols(pair_cat, accel_cat):\n",
    "    #When possible, this function adds acceleration data to the column in \n",
    "    indices = np.intersect1d(pair_cat['source_id_bg'],accel_cat['source_id_edr3'], return_indices = True)\n",
    "    \n",
    "    accel_ra_to_add = np.empty(len(pair_cat))\n",
    "    accel_dec_to_add = np.empty(len(pair_cat))\n",
    "    \n",
    "    accel_ra_err_to_add = np.empty(len(pair_cat))\n",
    "    accel_dec_err_to_add = np.empty(len(pair_cat))\n",
    "    \n",
    "    #Initialize accelerations with NaN values\n",
    "    accel_ra_to_add[:] = np.nan\n",
    "    accel_dec_to_add[:] = np.nan\n",
    "    \n",
    "    accel_ra_err_to_add[:] = np.nan\n",
    "    accel_dec_err_to_add[:] = np.nan\n",
    "    \n",
    "    for i in range(len(indices[1])):\n",
    "        accel_ra_to_add[indices[1][i]] = accel_cat['accel_ra'].iloc[indices[2][i]]\n",
    "        accel_dec_to_add[indices[1][i]] = accel_cat['accel_dec'].iloc[indices[2][i]]\n",
    "        \n",
    "        accel_ra_err_to_add[indices[1][i]] = accel_cat['accel_ra_error'].iloc[indices[2][i]]\n",
    "        accel_dec_err_to_add[indices[1][i]] = accel_cat['accel_dec_error'].iloc[indices[2][i]]\n",
    "        \n",
    "    pair_cat['accel_ra_bg'] = accel_ra_to_add\n",
    "    pair_cat['accel_ra_error_bg'] = accel_ra_err_to_add\n",
    "    \n",
    "    pair_cat['accel_dec_bg'] = accel_dec_to_add\n",
    "    pair_cat['accel_dec_error_bg'] = accel_dec_err_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:17:32.161222Z",
     "iopub.status.busy": "2022-01-25T22:17:32.160847Z",
     "iopub.status.idle": "2022-01-25T22:17:32.319105Z",
     "shell.execute_reply": "2022-01-25T22:17:32.318764Z",
     "shell.execute_reply.started": "2022-01-25T22:17:32.161184Z"
    }
   },
   "outputs": [],
   "source": [
    "add_accel_cols(pair_cat, accel_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export catalogues to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "accel_cat_name = 'accels_' + str(healpix_edr3_start[current_index]) +'-'+ str(healpix_edr3_end[current_index])\n",
    "accel_cat.to_csv('./acceleration_catalogue/'+ accel_cat_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_cat_name = 'pairs_' + str(healpix_edr3_start[current_index]) +'-'+ str(healpix_edr3_end[current_index])\n",
    "pair_cat.to_csv('./accidental_pairs/' + pair_cat_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute acceleration statistics and save histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:18:45.654144Z",
     "iopub.status.busy": "2022-01-25T22:18:45.653777Z",
     "iopub.status.idle": "2022-01-25T22:18:49.901462Z",
     "shell.execute_reply": "2022-01-25T22:18:49.901156Z",
     "shell.execute_reply.started": "2022-01-25T22:18:45.654110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232412"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_execute(tab_acc_stat, current_index, n_iter=10, flag_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
